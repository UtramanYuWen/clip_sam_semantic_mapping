#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ã€æ¨¡å—ã€‘CLIPæˆ¿é—´å’Œç‰©ä½“åˆ†ç±»å™¨
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ã€åŠŸèƒ½æè¿°ã€‘
åŸºäºOpenAIçš„CLIPè§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œè¿›è¡Œæˆ¿é—´ç±»å‹å’Œç‰©ä½“åˆ†ç±»ï¼š
  âœ“ æˆ¿é—´è¯†åˆ«: 8ç§æˆ¿é—´ç±»å‹åˆ†ç±» (å®¢å…ã€å§å®¤ã€å¨æˆ¿ç­‰)
  âœ“ ç‰©ä½“åˆ†ç±»: æ‰©å±•æ”¯æŒé€šç”¨ç‰©ä½“åˆ†ç±»
  âœ“ æ–‡æœ¬ç‰¹å¾é¢„è®¡ç®—: åŠ é€Ÿæ¨ç†ï¼Œå‡å°‘å†…å­˜å ç”¨
  âœ“ å¤šè¯­è¨€æ”¯æŒ: ä¸­æ–‡æç¤ºè¯ä¼˜åŒ–è¯†åˆ«æ•ˆæœ

ã€ä¸»è¦ç±»ã€‘
  - CLIPRoomClassifier: æˆ¿é—´å’Œç‰©ä½“åˆ†ç±»å™¨

ã€æ ¸å¿ƒæ–¹æ³•ã€‘
  - __init__: åˆå§‹åŒ–æ¨¡å‹ï¼ŒåŠ è½½é…ç½®
  - _precompute_text_features: é¢„è®¡ç®—æ–‡æœ¬ç‰¹å¾ä»¥åŠ é€Ÿæ¨ç†
  - classify_room: è¯†åˆ«æˆ¿é—´ç±»å‹
  - classify_objects: åˆ†ç±»å›¾åƒä¸­çš„ç‰©ä½“
  - get_feature_similarity: è®¡ç®—å›¾åƒç‰¹å¾ä¸å„ç±»åˆ«çš„ç›¸ä¼¼åº¦

ã€å·¥ä½œåŸç†ã€‘
  1. åŠ è½½CLIPæ¨¡å‹ (ViT-B/32 æˆ–å…¶ä»–ç‰ˆæœ¬)
  2. é¢„è®¡ç®—æˆ¿é—´æè¿°çš„æ–‡æœ¬ç‰¹å¾å¹¶ç¼“å­˜
  3. å¤„ç†è¾“å…¥å›¾åƒï¼Œæå–è§†è§‰ç‰¹å¾
  4. è®¡ç®—å›¾åƒç‰¹å¾ä¸å„æˆ¿é—´ç±»å‹çš„ç›¸ä¼¼åº¦
  5. è¿”å›ç½®ä¿¡åº¦æœ€é«˜çš„æˆ¿é—´ç±»å‹å’Œåˆ†æ•°

ã€æ”¯æŒçš„æˆ¿é—´ç±»å‹ã€‘
  - English: living_room, bedroom, kitchen, bathroom, study_room, dining_room, balcony, entrance
  - ä¸­æ–‡: å®¢å…ã€å§å®¤ã€å¨æˆ¿ã€å«ç”Ÿé—´ã€ä¹¦æˆ¿ã€é¤å…ã€é˜³å°ã€ç„å…³

ã€æ¨¡å‹é…ç½®ã€‘
  - æ¨¡å‹ç‰ˆæœ¬: ViT-B/32 (é»˜è®¤) æˆ– ViT-L/14
  - è®¾å¤‡: è‡ªåŠ¨æ£€æµ‹ (GPUä¼˜å…ˆï¼ŒCPUå¤‡é€‰)
  - ç½®ä¿¡åº¦é˜ˆå€¼: å¯é…ç½® (é»˜è®¤0.12)
  - æç¤ºè¯è¯­è¨€: ä¸­æ–‡ä¼˜åŒ–

ã€æ€§èƒ½æŒ‡æ ‡ã€‘
  - å•å¼ å›¾åƒæ¨ç†æ—¶é—´: ~50-100ms (GPU)
  - ååé‡: 10-20å¼ /ç§’ (GPU)
  - å†…å­˜å ç”¨: ~2GB (åŒ…å«æ¨¡å‹)

ã€é…ç½®å‚æ•°ã€‘
  ä» config/clip_sam_config.yaml è¯»å–:
  - clip.model_name: æ¨¡å‹ç‰ˆæœ¬é€‰æ‹©
  - clip.device: æ¨ç†è®¾å¤‡ (cuda/cpu/auto)
  - clip.confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
  - room_types: æˆ¿é—´ç±»å‹åˆ—è¡¨
  - clip_prompts.per_room_prompts: è‡ªå®šä¹‰æˆ¿é—´æç¤ºè¯

ã€ä½¿ç”¨ç¤ºä¾‹ã€‘
  classifier = CLIPRoomClassifier(config_path)
  room_type, confidence = classifier.classify_room(image_array)
  
ã€ç‰ˆæœ¬ä¿¡æ¯ã€‘
  - å½“å‰ç‰ˆæœ¬: 3.0
  - æœ€åæ›´æ–°: 2025å¹´12æœˆ9æ—¥
"""

import torch
import clip
import numpy as np
from PIL import Image as PILImage
from typing import Dict, List, Tuple
import yaml
import os

class CLIPRoomClassifier:
    def __init__(self, config_path: str = None):
        # åŠ è½½é…ç½®
        if config_path is None:
            current_dir = os.path.dirname(os.path.abspath(__file__))
            config_path = os.path.join(current_dir, "../config/clip_sam_config.yaml")
        
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        # è®¾å¤‡é…ç½®
        device_config = self.config['clip']['device']
        if device_config == "auto":
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device_config
        
        print(f"ğŸ”¥ CLIPä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åŠ è½½CLIPæ¨¡å‹
        model_name = self.config['clip']['model_name']
        print(f"ğŸ”„ åŠ è½½CLIPæ¨¡å‹: {model_name}")
        self.clip_model, self.clip_preprocess = clip.load(model_name, device=self.device)
        print("âœ… CLIPæ¨¡å‹åŠ è½½å®Œæˆ")
        
        # æˆ¿é—´ç±»å‹æ˜ å°„
        self.room_types_en = self.config['room_types']['english']
        self.room_types_cn = self.config['room_types']['chinese']
        self.room_mapping = dict(zip(self.room_types_en, self.room_types_cn))
        
        # é¢„è®¡ç®—æ–‡æœ¬ç‰¹å¾
        self.precomputed_features = self._precompute_text_features()
        
        # ğŸ”§ æ–°å¢ï¼šCLIPæ¨¡å‹çš„ç›´æ¥è®¿é—®å±æ€§ (ç”¨äºå…¼å®¹)
        self.model = self.clip_model
        self.preprocess = self.clip_preprocess
    
    def _precompute_text_features(self) -> Dict[str, torch.Tensor]:
        """é¢„è®¡ç®—æˆ¿é—´ç±»å‹çš„æ–‡æœ¬ç‰¹å¾"""
        print("ğŸ”„ é¢„è®¡ç®—CLIPæ–‡æœ¬ç‰¹å¾...")
        
        # ğŸ“Œ ä¼˜å…ˆè¯»å– per_room_prompts é…ç½®
        per_room_prompts = self.config.get('clip_prompts', {}).get('per_room_prompts', {})
        
        # é»˜è®¤æˆ¿é—´æè¿°ï¼ˆä»…åœ¨ per_room_prompts ä¸­ä¸å­˜åœ¨æ—¶ä½¿ç”¨ï¼‰
        descriptions = self._get_default_descriptions()
        
        precomputed_features = {}
        
        for room_type in self.room_types_en:
            room_key = room_type.replace(' ', '_')
            chinese_room = self.room_mapping.get(room_type, room_key)
            
            # ğŸ“Œ ä¼˜å…ˆä½¿ç”¨ per_room_prompts ä¸­è¯¥æˆ¿é—´çš„æç¤ºè¯
            if chinese_room in per_room_prompts:
                room_descriptions = per_room_prompts[chinese_room]
                print(f"  âœ¨ä½¿ç”¨è‡ªå®šä¹‰æç¤ºè¯: {chinese_room}")
            else:
                room_descriptions = descriptions.get(room_key, [f"a {room_type}"])
            
            # å¯¹æ¯ä¸ªæˆ¿é—´çš„å¤šä¸ªæè¿°è®¡ç®—å¹³å‡ç‰¹å¾
            text_tokens = clip.tokenize(room_descriptions).to(self.device)
            
            with torch.no_grad():
                text_features = self.clip_model.encode_text(text_tokens)
                # å¹³å‡æ‰€æœ‰æè¿°çš„ç‰¹å¾å¹¶å½’ä¸€åŒ–
                avg_features = text_features.mean(dim=0, keepdim=True)
                avg_features = avg_features / avg_features.norm(dim=-1, keepdim=True)
            
            precomputed_features[room_type] = avg_features
        
        print("âœ… CLIPæ–‡æœ¬ç‰¹å¾é¢„è®¡ç®—å®Œæˆ")
        return precomputed_features
    
    def _get_default_descriptions(self) -> Dict[str, List[str]]:
        """é»˜è®¤æˆ¿é—´æè¿°"""
        return {
            "living_room": ["a living room", "living room with sofa", "family room"],
            "bedroom": ["a bedroom", "sleeping room with bed", "bedroom interior"], 
            "kitchen": ["a kitchen", "cooking area", "kitchen with stove"],
            "bathroom": ["a bathroom", "toilet room", "washroom"],
            "balcony": ["a balcony", "terrace", "outdoor balcony"],
            "study_room": ["a study room", "office room", "library"],
            "dining_room": ["a dining room", "eating area", "dining space"],
            "entrance_hall": ["an entrance hall", "foyer", "entry way"]
        }
    
    def classify_room(self, image: PILImage.Image, 
                     confidence_threshold: float = 0.2) -> Tuple[str, float]:
        """
        ä½¿ç”¨CLIPåˆ†ç±»æˆ¿é—´ç±»å‹
        
        Args:
            image: PILå›¾åƒ
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
            
        Returns:
            (ä¸­æ–‡æˆ¿é—´å, ç½®ä¿¡åº¦åˆ†æ•°)
        """
        try:
            # é¢„å¤„ç†å›¾åƒ
            preprocessed_image = self.clip_preprocess(image).unsqueeze(0).to(self.device)
            
            # æå–å›¾åƒç‰¹å¾
            with torch.no_grad():
                image_features = self.clip_model.encode_image(preprocessed_image)
                image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            
            # ä¸é¢„è®¡ç®—çš„æˆ¿é—´ç‰¹å¾æ¯”è¾ƒ
            best_room = "living room"  # é»˜è®¤å€¼
            best_similarity = -1
            
            similarities = {}
            for room_type, text_features in self.precomputed_features.items():
                similarity = (image_features @ text_features.T).item()
                similarities[room_type] = similarity
                
                if similarity > best_similarity:
                    best_similarity = similarity
                    best_room = room_type
            
            # è½¬æ¢ä¸ºä¸­æ–‡
            chinese_room = self.room_mapping.get(best_room, "æœªçŸ¥")
            
            # ç½®ä¿¡åº¦æ£€æŸ¥
            if best_similarity < confidence_threshold:
                chinese_room = "æœªçŸ¥"
            
            return chinese_room, best_similarity
            
        except Exception as e:
            print(f"âŒ CLIPåˆ†ç±»é”™è¯¯: {e}")
            return "æœªçŸ¥", 0.0
    
    def classify_with_prompts(self, image: PILImage.Image, prompts: List[str], 
                            confidence_threshold: float = 0.15) -> Tuple[str, float]:
        """
        ğŸ”§ æ–°å¢ï¼šé€šç”¨CLIPåˆ†ç±»æ–¹æ³• - æ”¯æŒè‡ªå®šä¹‰æç¤ºè¯åˆ—è¡¨
        
        Args:
            image: PILå›¾åƒ
            prompts: æç¤ºè¯åˆ—è¡¨ (å¦‚ ["a sofa", "furniture sofa", "living room sofa"])
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
            
        Returns:
            (best_prompt, confidence) å…ƒç»„
        """
        try:
            if not prompts:
                return "unknown", 0.0
            
            # é¢„å¤„ç†å›¾åƒ
            preprocessed_image = self.clip_preprocess(image).unsqueeze(0).to(self.device)
            
            # ä½¿ç”¨CLIPè¿›è¡Œæ¨ç†
            with torch.no_grad():
                # å¯¹æ‰€æœ‰æç¤ºè¯è¿›è¡Œtokenization
                text_tokens = clip.tokenize(prompts).to(self.device)
                
                # è·å–å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾
                image_features = self.clip_model.encode_image(preprocessed_image)
                text_features = self.clip_model.encode_text(text_tokens)
                
                # å½’ä¸€åŒ–ç‰¹å¾
                image_features = image_features / image_features.norm(dim=-1, keepdim=True)
                text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                
                # è®¡ç®—ç›¸ä¼¼åº¦
                logits_per_image = (image_features @ text_features.T) * 100  # æ¸©åº¦ç¼©æ”¾
                probs = logits_per_image.softmax(dim=-1).cpu().numpy()[0]
                
                # æ‰¾åˆ°æœ€ä½³åŒ¹é…
                best_idx = np.argmax(probs)
                best_confidence = float(probs[best_idx])
                best_prompt = prompts[best_idx]
                
                if best_confidence >= confidence_threshold:
                    return best_prompt, best_confidence
                else:
                    return "unknown", best_confidence
                    
        except Exception as e:
            print(f"âŒ CLIPé€šç”¨åˆ†ç±»é”™è¯¯: {e}")
            return "unknown", 0.0
    
    def classify_object(self, image: PILImage.Image, object_prompts: List[str], 
                       confidence_threshold: float = 0.2) -> Tuple[str, float]:
        """
        ğŸ”§ æ–°å¢ï¼šä¸“é—¨çš„ç‰©å“åˆ†ç±»æ–¹æ³•
        
        Args:
            image: PILå›¾åƒ
            object_prompts: ç‰©å“æç¤ºè¯åˆ—è¡¨
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
            
        Returns:
            (ç‰©å“ç±»åˆ«, ç½®ä¿¡åº¦)
        """
        try:
            # å¢å¼ºç‰©å“æç¤ºè¯ï¼Œæé«˜è¯†åˆ«å‡†ç¡®æ€§
            enhanced_prompts = []
            for prompt in object_prompts:
                enhanced_prompts.extend([
                    prompt,
                    f"a {prompt}",
                    f"{prompt} in a room",
                    f"furniture {prompt}",
                    f"household {prompt}"
                ])
            
            # å»é‡å¹¶é™åˆ¶æ•°é‡ï¼ˆé¿å…CLIPå¤„ç†è¿‡å¤štokenï¼‰
            enhanced_prompts = list(dict.fromkeys(enhanced_prompts))[:20]
            
            result, confidence = self.classify_with_prompts(
                image, enhanced_prompts, confidence_threshold
            )
            
            # æ¸…ç†ç»“æœï¼Œæå–æ ¸å¿ƒç‰©å“åç§°
            if result != "unknown":
                # æå–åŸå§‹ç‰©å“åç§°
                for original_prompt in object_prompts:
                    if original_prompt in result:
                        return original_prompt, confidence
            
            return result, confidence
            
        except Exception as e:
            print(f"âŒ ç‰©å“åˆ†ç±»é”™è¯¯: {e}")
            return "unknown", 0.0
    
    def get_room_similarities(self, image: PILImage.Image) -> Dict[str, float]:
        """è·å–æ‰€æœ‰æˆ¿é—´ç±»å‹çš„ç›¸ä¼¼åº¦åˆ†æ•°"""
        try:
            preprocessed_image = self.clip_preprocess(image).unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                image_features = self.clip_model.encode_image(preprocessed_image)
                image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            
            similarities = {}
            for room_type, text_features in self.precomputed_features.items():
                similarity = (image_features @ text_features.T).item()
                chinese_room = self.room_mapping.get(room_type, room_type)
                similarities[chinese_room] = similarity
            
            return similarities
            
        except Exception as e:
            print(f"âŒ è·å–ç›¸ä¼¼åº¦é”™è¯¯: {e}")
            return {}
    
    def batch_classify_rooms(self, images: List[PILImage.Image], 
                           confidence_threshold: float = 0.2) -> List[Tuple[str, float]]:
        """
        ğŸ”§ æ–°å¢ï¼šæ‰¹é‡æˆ¿é—´åˆ†ç±» - æé«˜æ•ˆç‡
        
        Args:
            images: PILå›¾åƒåˆ—è¡¨
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
            
        Returns:
            [(æˆ¿é—´å, ç½®ä¿¡åº¦), ...] åˆ—è¡¨
        """
        try:
            if not images:
                return []
            
            # æ‰¹é‡é¢„å¤„ç†å›¾åƒ
            batch_images = torch.stack([
                self.clip_preprocess(img) for img in images
            ]).to(self.device)
            
            results = []
            
            with torch.no_grad():
                # æ‰¹é‡æå–å›¾åƒç‰¹å¾
                image_features = self.clip_model.encode_image(batch_images)
                image_features = image_features / image_features.norm(dim=-1, keepdim=True)
                
                for i, img_features in enumerate(image_features):
                    best_room = "living room"
                    best_similarity = -1
                    
                    # ä¸é¢„è®¡ç®—ç‰¹å¾æ¯”è¾ƒ
                    for room_type, text_features in self.precomputed_features.items():
                        similarity = (img_features @ text_features.T).item()
                        
                        if similarity > best_similarity:
                            best_similarity = similarity
                            best_room = room_type
                    
                    # è½¬æ¢ç»“æœ
                    chinese_room = self.room_mapping.get(best_room, "æœªçŸ¥")
                    if best_similarity < confidence_threshold:
                        chinese_room = "æœªçŸ¥"
                    
                    results.append((chinese_room, best_similarity))
            
            return results
            
        except Exception as e:
            print(f"âŒ æ‰¹é‡åˆ†ç±»é”™è¯¯: {e}")
            return [("æœªçŸ¥", 0.0)] * len(images)
    
    def get_device(self) -> str:
        """è·å–å½“å‰ä½¿ç”¨çš„è®¾å¤‡"""
        return self.device
    
    def get_model_info(self) -> Dict[str, str]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            "model_name": self.config['clip']['model_name'],
            "device": self.device,
            "supported_rooms": len(self.room_types_cn),
            "room_types": self.room_types_cn
        }

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    classifier = CLIPRoomClassifier()
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒ
    test_image = PILImage.new('RGB', (224, 224), color='white')
    
    # æµ‹è¯•æˆ¿é—´åˆ†ç±»
    result, confidence = classifier.classify_room(test_image)
    print(f"æˆ¿é—´åˆ†ç±»æµ‹è¯•: {result}, ç½®ä¿¡åº¦: {confidence:.3f}")
    
    # æµ‹è¯•é€šç”¨åˆ†ç±»
    test_prompts = ["a sofa", "a chair", "a table"]
    obj_result, obj_confidence = classifier.classify_with_prompts(test_image, test_prompts)
    print(f"é€šç”¨åˆ†ç±»æµ‹è¯•: {obj_result}, ç½®ä¿¡åº¦: {obj_confidence:.3f}")
    
    # æµ‹è¯•ç‰©å“åˆ†ç±»
    object_prompts = ["sofa", "chair"]
    furniture_result, furniture_confidence = classifier.classify_object(test_image, object_prompts)
    print(f"ç‰©å“åˆ†ç±»æµ‹è¯•: {furniture_result}, ç½®ä¿¡åº¦: {furniture_confidence:.3f}")
    
    # æµ‹è¯•ç›¸ä¼¼åº¦
    similarities = classifier.get_room_similarities(test_image)
    print("æˆ¿é—´ç›¸ä¼¼åº¦:", similarities)
    
    # æµ‹è¯•æ‰¹é‡åˆ†ç±»
    batch_results = classifier.batch_classify_rooms([test_image, test_image])
    print("æ‰¹é‡åˆ†ç±»æµ‹è¯•:", batch_results)
    
    # è¾“å‡ºæ¨¡å‹ä¿¡æ¯
    model_info = classifier.get_model_info()
    print("æ¨¡å‹ä¿¡æ¯:", model_info)