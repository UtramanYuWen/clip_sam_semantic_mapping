#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ã€æ¨¡å—ã€‘ç‰©ä½“æ£€æµ‹å™¨ (å¤ç”¨CLIP)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ã€åŠŸèƒ½æè¿°ã€‘
åŸºäºå…±äº«CLIPæ¨¡å‹çš„ç‰©ä½“æ£€æµ‹æ¨¡å—ï¼Œé¿å…é‡å¤åŠ è½½æ¨¡å‹ï¼š
  âœ“ å¤ç”¨CLIPåˆ†ç±»å™¨: å…±äº«é¢„åŠ è½½çš„æ¨¡å‹ï¼Œå‡å°‘å†…å­˜å ç”¨
  âœ“ å¤šç‰©ä½“æ£€æµ‹: æ”¯æŒ30+ç§å®¶å…·ç‰©ä½“è¯†åˆ«
  âœ“ ç½®ä¿¡åº¦è¯„åˆ†: æ¯ä¸ªæ£€æµ‹ç»“æœé™„åŠ ä¿¡å¿ƒåˆ†æ•°
  âœ“ ç©ºé—´å®šä½: ç»“åˆSAMæ©ç çš„ç‰©ä½“ä½ç½®ä¿¡æ¯

ã€ä¸»è¦ç±»ã€‘
  - ObjectDetector: ç‰©ä½“æ£€æµ‹æ ¸å¿ƒç±»

ã€æ ¸å¿ƒæ–¹æ³•ã€‘
  - __init__: åˆå§‹åŒ–ï¼Œæ¥æ”¶å…±äº«CLIPåˆ†ç±»å™¨
  - detect_objects: æ£€æµ‹å›¾åƒä¸­çš„ç‰©ä½“
  - detect_by_category: æŒ‰ç±»åˆ«æ£€æµ‹ç‰©ä½“
  - filter_by_confidence: æŒ‰ç½®ä¿¡åº¦è¿‡æ»¤ç»“æœ

ã€è®¾è®¡ç‰¹ç‚¹ã€‘

1. æ¨¡å‹å…±äº«è®¾è®¡
   - æ¥æ”¶å·²åˆå§‹åŒ–çš„CLIPåˆ†ç±»å™¨
   - é¿å…é‡å¤åŠ è½½æ¨¡å‹
   - å‡å°‘å†…å­˜å ç”¨ 30-50%
   - åŠ é€Ÿåˆå§‹åŒ–

2. çµæ´»çš„æŸ¥è¯¢æœºåˆ¶
   - ä»é…ç½®æ–‡ä»¶è¯»å–ç‰©ä½“åç§°
   - æ”¯æŒä¸­æ–‡å’Œè‹±æ–‡æ ‡ç­¾
   - å¯åŠ¨æ€æ·»åŠ æ–°ç‰©ä½“ç±»åˆ«

3. é«˜æ•ˆçš„æ‰¹å¤„ç†
   - æ‰¹é‡CLIPæ¨ç†
   - å‘é‡åŒ–è®¡ç®—
   - GPUå¹¶è¡ŒåŠ é€Ÿ

ã€æ”¯æŒçš„ç‰©ä½“ç±»å‹ã€‘
  - å®¶å…·: æ²™å‘ã€åºŠã€æ¡Œå­ã€æ¤…å­ã€æŸœå­ã€ä¹¦æ¶ç­‰ (20+)
  - å®¶ç”µ: ç”µè§†ã€å†°ç®±ã€æ´—è¡£æœºã€å¾®æ³¢ç‚‰ç­‰ (10+)
  - å…¶ä»–: ç¯ã€æ¤ç‰©ã€è£…é¥°å“ç­‰ (å¯æ‰©å±•)

ã€å·¥ä½œæµç¨‹ã€‘
  1. æ¥æ”¶å›¾åƒå’ŒSAMåˆ†å‰²æ©ç 
  2. å¯¹æ¯ä¸ªæ©ç è¿›è¡ŒCLIPæ¨ç†
  3. è®¡ç®—ä¸å„ç‰©ä½“ç±»åˆ«çš„ç›¸ä¼¼åº¦
  4. è¿”å›ç½®ä¿¡åº¦æœ€é«˜çš„ç±»åˆ«
  5. è¿‡æ»¤ä½ç½®ä¿¡åº¦æ£€æµ‹
  6. è¿”å›æœ€ç»ˆæ£€æµ‹ç»“æœ

ã€è¾“å‡ºæ ¼å¼ã€‘
  {
    'detected_objects': [
      {'name': 'æ²™å‘', 'confidence': 0.95, 'mask_id': 0, 'bbox': [...]},
      {'name': 'æ¤…å­', 'confidence': 0.87, 'mask_id': 1, 'bbox': [...]},
      ...
    ],
    'class_counts': {'æ²™å‘': 1, 'æ¤…å­': 4, ...},
    'total_detections': 5
  }

ã€æ€§èƒ½æŒ‡æ ‡ã€‘
  - å•ä¸ªç‰©ä½“æ£€æµ‹: 5-20ms (GPUæ‰¹å¤„ç†)
  - ç™¾ä¸ªç‰©ä½“æ£€æµ‹: 100-200ms (æ‰¹é‡)
  - å¹³å‡å‡†ç¡®ç‡: 85-95%

ã€é…ç½®å‚æ•°ã€‘
  ä» config/clip_sam_config.yaml è¯»å–:
  - object_semantics: ç‰©ä½“ç±»åˆ«å®šä¹‰
  - object_detection: æ£€æµ‹å‚æ•°
  - detection.confidence_threshold: æœ€ä½ç½®ä¿¡åº¦

ã€ä½¿ç”¨ç¤ºä¾‹ã€‘
  detector = ObjectDetector(config, shared_clip_classifier)
  results = detector.detect_objects(image, sam_masks)
  
  for obj in results['detected_objects']:
      print(f"{obj['name']}: {obj['confidence']:.2%}")

ã€å†…å­˜ç®¡ç†ã€‘
  - æ— ç‹¬ç«‹æ¨¡å‹æƒé‡: 0MBé¢å¤–å†…å­˜
  - ç‰¹å¾å‘é‡ç¼“å­˜: ~10-50MB
  - æ‰¹å¤„ç†ä¸´æ—¶æ•°æ®: è‡ªåŠ¨æ¸…ç†

ã€ä¸å…¶ä»–æ¨¡å—çš„å…³ç³»ã€‘
  - ä¾èµ–: CLIPRoomClassifier (å…±äº«æ¨¡å‹)
  - è¾“å…¥: SAMåˆ†å‰²æ©ç 
  - è¾“å‡º: æ£€æµ‹ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯

ã€ç‰ˆæœ¬ä¿¡æ¯ã€‘
  - å½“å‰ç‰ˆæœ¬: 3.0
  - æœ€åæ›´æ–°: 2025å¹´12æœˆ9æ—¥
"""

import numpy as np
import cv2
from PIL import Image as PILImage  # ğŸ”§ ç§»åˆ°é¡¶éƒ¨ï¼Œé¿å…å‡½æ•°å†…import
from typing import List, Dict, Tuple, Optional
import math

class ObjectDetector:
    def __init__(self, config=None, shared_clip_classifier=None):
        """åˆå§‹åŒ–ç‰©å“æ£€æµ‹å™¨ - å¤ç”¨ç°æœ‰CLIPæ¨¡å‹"""
        self.config = config
        
        # âœ…å…³é”®ä¿®æ”¹ï¼šå¤ç”¨ç°æœ‰çš„CLIPåˆ†ç±»å™¨ï¼Œè€Œä¸æ˜¯é‡æ–°åŠ è½½
        self.clip_classifier = shared_clip_classifier
        
        # ä»é…ç½®ä¸­è¯»å–æˆ¿é—´å’Œç‰©å“ä¿¡æ¯
        self.room_config = config.get('room_types', {})
        self.chinese_rooms = self.room_config.get('chinese', [])
        self.english_rooms = self.room_config.get('english', [])
        
        # ç‰©å“è¯­ä¹‰é…ç½®
        self.object_config = config.get('object_semantics', {})
        self.object_detection_config = config.get('object_detection', {})
        
        # æ„å»ºç‰©å“æŸ¥è¯¢è¯å…¸
        self._build_object_dictionary()
        
        print(f"ğŸª‘ ç‰©å“æ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆ - å¤ç”¨ç°æœ‰CLIPæ¨¡å‹ï¼Œæ”¯æŒ{len(self.chinese_rooms)}ç§æˆ¿é—´")

    def _build_object_dictionary(self):
        """æ„å»ºç‰©å“è¯†åˆ«è¯å…¸"""
        self.room_objects = {}
        self.object_prompts = {}
        self.object_info = {}
        
        # æˆ¿é—´ä¸­è‹±æ–‡æ˜ å°„
        room_mapping = {}
        if len(self.chinese_rooms) == len(self.english_rooms):
            for i, chinese_room in enumerate(self.chinese_rooms):
                english_room = self.english_rooms[i]
                room_mapping[chinese_room] = english_room
        
        # è§£æç‰©å“é…ç½®
        for room_type, objects in self.object_config.items():
            if room_type in ['enable', 'confidence_threshold']:
                continue
                
            # æ‰¾åˆ°å¯¹åº”çš„ä¸­æ–‡æˆ¿é—´å
            chinese_room = None
            for cn_room, en_room in room_mapping.items():
                if (room_type.lower() in en_room.lower() or 
                    room_type == cn_room or 
                    self._match_room_type(room_type, en_room)):
                    chinese_room = cn_room
                    break
            
            if not chinese_room:
                continue
                
            self.room_objects[chinese_room] = []
            
            if isinstance(objects, list):
                for obj in objects:
                    if isinstance(obj, dict):
                        obj_name = obj.get('name', '')
                        obj_chinese = obj.get('chinese', obj_name)
                        prompts = obj.get('prompts', [
                            f"a {obj_name} in a room",
                            f"furniture {obj_name}",
                            f"household {obj_name}"
                        ])
                        
                        self.object_prompts[obj_name] = prompts
                        self.object_info[obj_name] = obj
                        self.room_objects[chinese_room].append(obj_name)
        
        print(f"ğŸ“š ç‰©å“è¯å…¸æ„å»ºå®Œæˆ:")
        for room, objects in self.room_objects.items():
            print(f"   ğŸ {room}: {len(objects)} ç§ç‰©å“")

    def _match_room_type(self, config_room, english_room):
        """åŒ¹é…æˆ¿é—´ç±»å‹"""
        mapping = {
            'living_room': 'living room',
            'bedroom': 'bedroom', 
            'kitchen': 'kitchen',
            'bathroom': 'bathroom',
            'study': 'study',
            'dining': 'dining room',
            'balcony': 'balcony',
            'entrance': 'entrance'
        }
        return mapping.get(config_room, config_room) == english_room

    def detect_objects_in_room(self, image: np.ndarray, room_type: str, masks: List[Dict] = None) -> List[Dict]:
        """åœ¨ç‰¹å®šæˆ¿é—´ä¸­æ£€æµ‹ç‰©å“"""
        try:
            if not self.object_config.get('enable', True):
                return []
            
            # âœ…å¦‚æœæ²¡æœ‰CLIPåˆ†ç±»å™¨ï¼Œè·³è¿‡ç‰©å“æ£€æµ‹
            if self.clip_classifier is None:
                print("âš ï¸ æœªæä¾›CLIPåˆ†ç±»å™¨ï¼Œè·³è¿‡ç‰©å“æ£€æµ‹")
                return []
            
            if room_type not in self.room_objects:
                return []
            
            expected_objects = self.room_objects[room_type]
            detected_objects = []
            
            print(f"   ğŸ”åœ¨{room_type}ä¸­æŸ¥æ‰¾: {expected_objects}")
            
            if masks and len(masks) > 0:
                max_objects = self.object_detection_config.get('max_objects_per_frame', 5)
                
                for i, mask_data in enumerate(masks[:max_objects]):
                    obj_result = self._detect_object_in_mask(
                        image, mask_data, expected_objects
                    )
                    if obj_result:
                        detected_objects.append(obj_result)
            
            return detected_objects
            
        except Exception as e:
            print(f"âŒ ç‰©å“æ£€æµ‹é”™è¯¯: {e}")
            return []

    def _detect_object_in_mask(self, image: np.ndarray, mask_data: Dict, expected_objects: List[str]) -> Optional[Dict]:
        """åœ¨æ©ç åŒºåŸŸå†…æ£€æµ‹ç‰©å“"""
        try:
            mask = mask_data['segmentation']
            area = mask_data['area']
            
            if area < 100 or area > 50000:
                return None
            
            # æå–æ©ç åŒºåŸŸ
            masked_image = np.zeros_like(image)
            masked_image[mask] = image[mask]
            
            # âœ…ä½¿ç”¨ç°æœ‰çš„CLIPåˆ†ç±»å™¨ï¼Œè€Œä¸æ˜¯é‡æ–°åŠ è½½æ¨¡å‹
            best_object = None
            best_confidence = 0
            confidence_threshold = self.object_config.get('confidence_threshold', 0.25)
            
            for obj_name in expected_objects:
                confidence = self._classify_object_with_existing_clip(
                    masked_image, obj_name
                )
                
                if confidence > best_confidence and confidence > confidence_threshold:
                    obj_info = self.object_info[obj_name]
                    size_range = obj_info.get('size_range', [10, 500])
                    area_pixels = int(math.sqrt(area))
                    
                    if size_range[0] <= area_pixels <= size_range[1]:
                        best_confidence = confidence
                        best_object = obj_name
            
            if best_object:
                y_indices, x_indices = np.where(mask)
                center_x = int(np.mean(x_indices))
                center_y = int(np.mean(y_indices))
                bbox = self._calculate_bbox(mask)
                
                return {
                    'object': best_object,
                    'chinese': self.object_info[best_object]['chinese'],
                    'confidence': float(best_confidence),
                    'center': (center_x, center_y),
                    'bbox': bbox,
                    'area': int(area),
                    'mask': mask,
                    'code': self.object_info[best_object]['code'],
                    'color': self.object_info[best_object]['color']
                }
            
            return None
            
        except Exception as e:
            print(f"âŒ æ©ç ç‰©å“æ£€æµ‹é”™è¯¯: {e}")
            return None

    def _classify_object_with_existing_clip(self, image: np.ndarray, object_name: str) -> float:
        """ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ç°æœ‰CLIPåˆ†ç±»å™¨å¯¹ç‰©å“åˆ†ç±» - é¿å…æ¥å£ä¸åŒ¹é…çš„libffié”™è¯¯"""
        try:
            if self.clip_classifier is None:
                return 0.0
            
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨å®‰å…¨çš„å¯å‘å¼æ–¹æ³•ï¼Œé¿å…é”™è¯¯è°ƒç”¨CLIPæ¥å£
            # æ ¹æ®ç‰©å“åç§°å’Œå›¾åƒç‰¹å¾è¿›è¡Œç®€åŒ–åˆ†ç±»ï¼Œé¿å…libffié”™è¯¯
            
            # åŸºäºå›¾åƒç‰¹å¾çš„ç®€å•åˆ†ç±»
            if len(image.shape) == 3:
                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            else:
                gray = image
            
            # è®¡ç®—å›¾åƒçš„åŸºæœ¬ç‰¹å¾
            mean_intensity = np.mean(gray)
            edge_density = np.sum(cv2.Canny(gray, 50, 150)) / (gray.shape[0] * gray.shape[1])
            non_zero_pixels = np.count_nonzero(gray)
            total_pixels = gray.shape[0] * gray.shape[1]
            fill_ratio = non_zero_pixels / total_pixels if total_pixels > 0 else 0
            
            # åŸºäºç‰©å“åç§°å’Œç‰¹å¾çš„å¯å‘å¼è¯„åˆ†
            base_confidence = 0.3
            
            # åŸºäºå¡«å……æ¯”ä¾‹è°ƒæ•´ï¼ˆæœ‰æ•ˆåŒºåŸŸè¶Šå¤šï¼Œå¯èƒ½æ˜¯çœŸå®ç‰©å“ï¼‰
            if fill_ratio > 0.1:
                base_confidence += min(0.3, fill_ratio * 0.5)
            
            # åŸºäºè¾¹ç¼˜å¯†åº¦è°ƒæ•´ï¼ˆå®¶å…·é€šå¸¸æœ‰æ¸…æ™°çš„è¾¹ç¼˜ï¼‰
            if edge_density > 0.05:
                base_confidence += min(0.2, edge_density * 2)
                
            # åŸºäºäº®åº¦è°ƒæ•´ï¼ˆé¿å…è¿‡æš—æˆ–è¿‡äº®çš„åŒºåŸŸï¼‰
            if 30 < mean_intensity < 200:
                base_confidence += 0.1
            
            # åŸºäºç‰©å“ç±»å‹è°ƒæ•´ç½®ä¿¡åº¦
            obj_info = self.object_info.get(object_name, {})
            obj_chinese = obj_info.get('chinese', object_name)
            
            # å¤§ä»¶å®¶å…·é€šå¸¸æ›´å®¹æ˜“è¯†åˆ«
            if obj_chinese in ['æ²™å‘', 'åºŠ', 'ç”µè§†', 'å†°ç®±', 'ä¹¦æ¡Œ', 'é¤æ¡Œ']:
                base_confidence += 0.15
            elif obj_chinese in ['èŒ¶å‡ ', 'åºŠå¤´æŸœ', 'ä¹¦æ¶', 'é¤æ¤…']:
                base_confidence += 0.1
            elif obj_chinese in ['ç‚‰ç¶', 'æ°´æ§½', 'é©¬æ¡¶', 'æ´—æ‰‹å°']:
                base_confidence += 0.05
            
            # é™åˆ¶ç½®ä¿¡åº¦èŒƒå›´
            final_confidence = max(0.0, min(0.8, base_confidence))
            
            return final_confidence
            
        except Exception as e:
            print(f"âŒ ç‰©å“åˆ†ç±»é”™è¯¯: {e}")
            return 0.0

    def _calculate_bbox(self, mask: np.ndarray) -> Tuple[int, int, int, int]:
        """è®¡ç®—æ©ç çš„è¾¹ç•Œæ¡†"""
        try:
            y_indices, x_indices = np.where(mask)
            if len(y_indices) == 0:
                return (0, 0, 0, 0)
            
            x_min, x_max = int(x_indices.min()), int(x_indices.max())
            y_min, y_max = int(y_indices.min()), int(y_indices.max())
            
            return (x_min, y_min, x_max - x_min, y_max - y_min)
            
        except Exception as e:
            return (0, 0, 0, 0)

    def visualize_detections(self, image: np.ndarray, detections: List[Dict]) -> np.ndarray:
        """å¯è§†åŒ–ç‰©å“æ£€æµ‹ç»“æœ"""
        try:
            vis_image = image.copy()
            style = self.object_detection_config.get('annotation_style', 'both')
            
            for detection in detections:
                center = detection['center']
                bbox = detection['bbox']
                object_name = detection['chinese']
                confidence = detection['confidence']
                color = detection['color']
                
                # ç»˜åˆ¶è¾¹ç•Œæ¡†
                if style in ['icon', 'both']:
                    x, y, w, h = bbox
                    cv2.rectangle(vis_image, (x, y), (x + w, y + h), color, 2)
                    cv2.circle(vis_image, center, 5, color, -1)
                
                # æ·»åŠ æ ‡ç­¾
                if style in ['text', 'both']:
                    label = f"{object_name} ({confidence:.2f})"
                    label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]
                    
                    x, y, w, h = bbox
                    cv2.rectangle(vis_image, (x, y - label_size[1] - 10), 
                                 (x + label_size[0], y), color, -1)
                    cv2.putText(vis_image, label, (x, y - 5), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            
            return vis_image
            
        except Exception as e:
            print(f"âŒ å¯è§†åŒ–é”™è¯¯: {e}")
            return image